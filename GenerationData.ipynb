{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/Data/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "########import##########\n",
    "from multiprocessing.dummy import Pool\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "import h5py\n",
    "import sys\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random multiplexes, check if the instance generated has super-diffusion or not and store the labeled examples in a list\n",
    "# input: N number of nodes per layer\n",
    "#        p1, p2 connection probabilities in layer one and two\n",
    "#       iteration number of random examples generated\n",
    "# output: data list of instances labeled; every element is composed by [p1, p2, Adj1, Adj2, SD] (SD==1 -> super-diffusion is present; SD==0 otherwise)\n",
    "def SuperDiffusionHappens(N, p1, p2, iteration):\n",
    "    \n",
    "    # list that will contain the labeled instances\n",
    "    data=[]\n",
    "\n",
    "    # generate n instances\n",
    "    for i in range(iteration):\n",
    "    \n",
    "        lambda1, lambda2, lambdaSup = 0, 0, 0\n",
    "        \n",
    "        ## create Multiplex\n",
    "        G1 = nx.erdos_renyi_graph(N,p1)\n",
    "        G2 = nx.erdos_renyi_graph(N,p2)\n",
    "\n",
    "        \n",
    "        ## algebraic connectivity of first layer. \n",
    "        L1 = nx.laplacian_matrix(G1).toarray()\n",
    "        eigVal1, eigVec1 = np.linalg.eig(L1)\n",
    "        eigVal1.sort()\n",
    "        for e in eigVal1:\n",
    "            if e>1e-10:\n",
    "                lambda1=e\n",
    "                break;\n",
    "\n",
    "                \n",
    "        ## algebraic connectivity of second layer.\n",
    "        L2 = nx.laplacian_matrix(G2).toarray()\n",
    "        eigVal2, eigVec2 = np.linalg.eig(L2)\n",
    "        eigVal2.sort()\n",
    "        for e in eigVal2:\n",
    "            if e>1e-10:\n",
    "                lambda2=e\n",
    "                break;\n",
    "\n",
    "\n",
    "        ## algebraic connectivity of superposition of layers. \n",
    "        LSup=0.5*(L1+L2)\n",
    "        eigValSup,eigVecSup=np.linalg.eig(LSup)\n",
    "        eigValSup.sort()\n",
    "        for e in eigValSup:\n",
    "            if e>1e-10:\n",
    "                lambdaSup=e\n",
    "                break;\n",
    "\n",
    "        # check SD presence and append the instance to data\n",
    "        if lambdaSup>=max(lambda1,lambda2):                  #S-D is present\n",
    "            data.append((p1, p2, nx.adjacency_matrix(G1).toarray(), nx.adjacency_matrix(G2).toarray(), 1))\n",
    "        else:                                                #S-D is NOT present\n",
    "            data.append((p1, p2, nx.adjacency_matrix(G1).toarray(), nx.adjacency_matrix(G2).toarray(), 0))\n",
    "                \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########initialization##########\n",
    "\n",
    "# list used for storing sequence of parameters fed to different threads\n",
    "params=[]\n",
    "\n",
    "# array that will collect the generated multiplex instances across the different threads\n",
    "data=[]\n",
    "\n",
    "# number of nodes in each layer of the multiplex\n",
    "N=50\n",
    "\n",
    "# CHANGE WITH ACTUAL NUMBER OF THREAD\n",
    "numThreads=6\n",
    "\n",
    "# Struct used for pass data to the diffusion function in threads \n",
    "SDParam = namedtuple(\"SDParam\", \"N P1 P2 iterations id_th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function called in the different threads, just call the SuperDiffusion and store the data\n",
    "def callSuperDiff(param):\n",
    "    for par in param:\n",
    "        dat=SuperDiffusionHappens(par[0], par[1], par[2], par[3])\n",
    "        data.extend(dat)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate p_samples^2 * num_samples random multiplex instances labeled with super-diffusion presence or absence\n",
    "# input: p_samples number of different connection probabilities generate\n",
    "#        p_min min connection probability used\n",
    "#        p_max max connection probability used\n",
    "#        num_samples number of samples generated with a specific pair of connection probabilites\n",
    "#        name name of the .h5 file storing the labeled instances\n",
    "def Generation(p_samples, p_min, p_max, num_samples, name):\n",
    "    # generate  p_samples^2 combinations of (p1, p2)\n",
    "    P1, P2 = np.meshgrid(np.linspace(p_min, p_max, p_samples), np.linspace(p_min, p_max, p_samples))\n",
    "    zipped = zip(P1.ravel(), P2.ravel())\n",
    "    listZipped=list(zipped)\n",
    "    \n",
    "    print(\"length meshgrid: \", len(listZipped))\n",
    "    \n",
    "    #number of combinations that every thread need to process \n",
    "    fract=math.ceil(len(listZipped)/numThreads)\n",
    "    print(\"meshgrid per thread: \", fract)\n",
    "\n",
    "    # generate list of parameters needed to feed the threads \n",
    "    #print(\"++++++++++++++++++++\")\n",
    "    for i in range(0, numThreads):\n",
    "        pars=[]\n",
    "        for lisZ in listZipped[i*fract:(i+1)*fract]:\n",
    "            pars.append(SDParam(N=N, P1=lisZ[0], P2=lisZ[1], iterations=num_samples, id_th=i))\n",
    "        #print(i*fract)\n",
    "        #print(len(pars))\n",
    "        params.append(pars)\n",
    "        #print(\"----------\")\n",
    "\n",
    "    # make the Pool of workers\n",
    "    pool = Pool(numThreads) \n",
    "\n",
    "    print(\"Number of processes: \", pool._processes)\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # start the workers\n",
    "    _ = pool.map(callSuperDiff, params)\n",
    "\n",
    "    # close the pool and wait for the work to finish \n",
    "    pool.close() \n",
    "    pool.join() \n",
    "\n",
    "    elapsedTime = datetime.datetime.now()-start\n",
    "    \n",
    "    print( \"generation of \", p_samples*p_samples*num_samples, \" \", name, \" examples took \", str(elapsedTime))\n",
    "    \n",
    "    # data structures used for collecting the instances\n",
    "    data_p1 = np.empty(shape=(p_samples*p_samples*num_samples))          # connection probability of layer 1\n",
    "    data_p2 = np.empty(shape=(p_samples*p_samples*num_samples))          # connection probability of layer 2\n",
    "    data_A1 = np.zeros(shape=(p_samples*p_samples*num_samples, N, N))    # adjacency matrix of layer 1\n",
    "    data_A2 = np.zeros(shape=(p_samples*p_samples*num_samples, N, N))    # adjacency matrix of layer 2\n",
    "    data_Y = np.zeros(shape=(p_samples*p_samples*num_samples))           # presence 1 (or not 0) of super-diffusion\n",
    "\n",
    "    # for each instance generated add p1, p2, A1, A2 and Y \n",
    "    i=0\n",
    "    for dat in data:\n",
    "        data_p1[i]=dat[0]\n",
    "        data_p2[i]=dat[1]\n",
    "        data_A1[i]=dat[2]\n",
    "        data_A2[i]=dat[3]\n",
    "        data_Y[i]=dat[4]\n",
    "        i=i+1\n",
    "\n",
    "    # open the .h5 file and write p1, p2, A1, A2 and Y in different datasets\n",
    "    hf = h5py.File(name + '.h5', 'w')\n",
    "    print(hf.name)\n",
    "    print(hf.create_dataset(name+'_p1',  data=data_p1))\n",
    "    print(hf.create_dataset(name+'_p2',  data=data_p2))\n",
    "    print(hf.create_dataset(name+'_A1',  data=data_A1))\n",
    "    print(hf.create_dataset(name+'_A2',  data=data_A2))\n",
    "    print(hf.create_dataset(name+'_Y', data=data_Y))\n",
    "\n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "params=[]\n",
    "# example generating random multiplexes from a distribution of 50 by 50 p1 and p2 points\n",
    "# both starting from 0.01 and ending to 0.99. for every possible combination generate 5 random instances\n",
    "Generation(50, 0.01, 0.99, 5, \"train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
